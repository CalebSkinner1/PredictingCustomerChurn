---
title: "Project 2 Write Up"
author: Kevin Puorro, Asitha Mudiyanselage, Caleb Skinner
format:
  pdf:
    toc: true
---

```{r libraries}
library("tidyverse")
library("tidymodels")
library("janitor")
library("xgboost")
```

There are 10,000 records in this data set, consisting of 6 continuous variables, two categorical variables, and three numeric binary variables.
There is no high correlation between any variables, and there are no missing values in this dataset. The scales of the CreditScore and EstimatedSalary differ from the other variables. This indicates that scaling the dataset is necessary before applying a machine learning model.

```{r}

#Let's look at the basic statistics of the data set

dat <- read_csv("CustomerChurn.csv")

# Remove  CustomerId and Surname

dat <- dat[,-c(1,2)]

summary(dat)

glimpse(dat)

#Let's look at the  correlation matrix

cust_r <- cor(dat[,-c(,2,3)])

corrplot(cust_r, method = "number",number.cex = 0.7)

# Count missing values

colSums(is.na(dat))

# Frequency count of the target (Exited) variable

count(dat, Exited)

```
Asitha


```{r data}
customer <- read_csv("CustomerChurn.csv") %>%
  clean_names() %>%
  select(-customer_id, -surname) %>%
  mutate(
    exited = factor(exited),
    has_cr_card = factor(has_cr_card),
    is_active_member = factor(is_active_member)) %>%
  na.omit()

# split data
set.seed(1128)
customer_split <- initial_split(customer, prop = .75, strata = exited)
customer_train <- training(customer_split)
customer_test <- testing(customer_split)

# cross validation folds
set.seed(1128)
customer_folds <- vfold_cv(customer_train, v = 5, strata = exited)
```

# First Part

*Pick any machine learning method we have covered to predict ‘Exited‘ based on the other variables (except CustomerId and Surname). Be sure to do a training and testing split. Whatever method you choose to use, be sure to tune the model. Comment on the accuracy and confusion matrix for both the training set and the testing set.*

Comparing methods -

* Boosting: 83.5% accuracy
* Support Vector Machine: 79.6% accuracy
* Logistic Regression: 82.14% accuracy
* Random Forest: 

Kevin Puorro

# Second Part

*Use Principal Component Analysis to reduce the number of features (again, do not use CustomerId or Surname). Choose only the number of PCs that capture 75% of the variability.*

# PCA Analysis

The 21.85 % of the variability observed in the data set is explained by component 1

The 38.74 % of the variability observed in the data set is explained by first two components

he 55.43 % of the variability observed in the data set is explained by first three components

he 71.97% of the variability observed in the data set is explained by first four components

he 88.43 % of the variability observed in the data set is explained by five two components

The rate of change in variance slightly drops up to the second component and then levels off up to component 5.Based on the scree plot, the first five components are enough to retain 75% of the variability of the data set. These components are then used to create the data set for the random forest machine learning model.



```{r}

# PCA Analysis

set.seed(1124)

dim(cust_dat)

pca_df <- cust_dat[,-c(2,3,8,9)]

dim(pca_df)

customer_pca <- princomp(pca_df[,-7], cor =T)

summary(customer_pca, loadings = T)

screeplot(customer_pca)


```
Asitha

# Third Part

*Redo the method you used in part 1 but this time use the PCs found in part 2 (only the PCs that account for 75% of the variability). Again, comment on the accuracy and confusion matrix for both the training and testing sets.*

```{r PCA}
# create pca df, converting binary variables into continuous
# remove geography because cannot be converted into ordinal values
customer2 <- customer %>%
  mutate(
    has_cr_card = if_else(has_cr_card == "1", 1, 0),
    is_active_member = if_else(is_active_member == "1", 1, 0),
    gender = if_else(gender == "Female", 0, 1),
    exited = if_else(exited == "1", 1, 0)) %>%
  select(-geography)

pca_recipe <- recipe(exited ~ ., data = customer2) |>
              step_normalize(all_numeric_predictors()) |>
              step_pca(all_predictors(), threshold = .75)

# Prep the recipe to estimate PCA components

customer_prep_pca <- prep(pca_recipe, training = customer2)

# Extract the PCA results

customer_pca <- bake(customer_prep_pca, customer2) %>%
  mutate(
    exited = factor(exited),
    geography = customer$geography)

# View the results
tidy(customer_prep_pca, number = 2, type = "variance") %>%
  filter(terms == "cumulative percent variance") %>%
  ggplot() +
  geom_line(aes(x = component, y = value)) +
  scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9)) +
  geom_hline(yintercept = 75, linetype = "dashed") +
  labs(x = "Component", y = "Cumulative Percent Variance")

set.seed(1128)
pca_split <- customer_pca |> initial_split(prop = .75, strata = exited)

pca_train <- pca_split |> training()

pca_test <- pca_split |> testing()
pca_folds <- vfold_cv(pca_train, v = 5)
```

```{r pca rf}
rf_spec <- rand_forest(trees = 100,
                       mtry = tune(),
                       min_n = tune()) %>%
  set_engine("ranger") %>%
  set_mode("classification")

rf_pca_rec <- recipe(exited ~ ., data = pca_train) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())

rf_tuning_grid <- grid_regular(
  mtry(range = c(3, 6)),
  min_n(range = c(40, 50)),
  levels = 10)

rf_pca_tune_results <- tune_grid(
  object = workflow() %>%
    add_recipe(rf_pca_rec) %>%
    add_model(rf_spec),
  resamples = pca_folds,
  grid = rf_tuning_grid,
  metrics = metric_set(accuracy))

best_params <- select_best(rf_pca_tune_results, "accuracy")
best_params

pca_rf_final <- finalize_workflow(
  workflow() %>%
    add_recipe(rf_pca_rec) %>%
    add_model(rf_spec),
  best_params) %>%
  fit(data = pca_train)

predictions <- augment(pca_rf_final, new_data = pca_test)

metrics(predictions, truth = exited, estimate = .pred_class)
conf_mat(predictions, truth = exited, estimate = .pred_class)
```


Caleb



