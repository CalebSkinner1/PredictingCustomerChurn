---
title: "Project 2 Write Up"
author: Kevin Puorro, Asitha Mudiyanselage, Caleb Skinner
format:
  pdf:
    toc: true
---

```{r libraries}
library("tidyverse"); theme_set(theme_minimal())
library("tidymodels")
library("janitor")
library("xgboost")
library("vip")
```

\newpage

There are 10,000 records in this data set, consisting of 6 continuous variables, two categorical variables, and three numeric binary variables.

```{r basic summary statistics, include = FALSE}

#Let's look at the basic statistics of the data set

dat <- read_csv("CustomerChurn.csv")

# Remove  CustomerId and Surname

dat <- dat[,-c(1,2)]

summary(dat)

glimpse(dat)

#Let's look at the  correlation matrix

cust_r <- cor(dat[,-c(1,2,4,5)])

corrplot(cust_r, method = "number",number.cex = 0.7)

# Count missing values

colSums(is.na(dat))

# Frequency count of the target (Exited) variable

count(dat, Exited)

```

```{r data}
customer <- read_csv("CustomerChurn.csv") %>%
  clean_names() %>%
  select(-customer_id, -surname) %>%
  mutate(
    exited = factor(exited),
    has_cr_card = factor(has_cr_card),
    is_active_member = factor(is_active_member)) %>%
  na.omit()

# split data
set.seed(1128)
customer_split <- initial_split(customer, prop = .75, strata = exited)
customer_train <- training(customer_split)
customer_test <- testing(customer_split)

# cross validation folds
set.seed(1128)
customer_folds <- vfold_cv(customer_train, v = 5, strata = exited)
```

# Predicint 

*Pick any machine learning method we have covered to predict ‘Exited‘ based on the other variables (except CustomerId and Surname). Be sure to do a training and testing split. Whatever method you choose to use, be sure to tune the model. Comment on the accuracy and confusion matrix for both the training set and the testing set.*

Comparing methods -

* Boosting: 83.5% accuracy
* Support Vector Machine: 79.6% accuracy
* Logistic Regression: 82.14% accuracy
* Random Forest: 

Kevin Puorro

\newpage

# PCA Analysis

*Use Principal Component Analysis to reduce the number of features (again, do not use CustomerId or Surname). Choose only the number of PCs that capture 75% of the variability.*

```{r PCA}
# create pca df, converting binary variables into continuous
# remove geography because cannot be converted into ordinal values
customer2 <- customer %>%
  mutate(
    has_cr_card = if_else(has_cr_card == "1", 1, 0),
    is_active_member = if_else(is_active_member == "1", 1, 0),
    gender = if_else(gender == "Female", 0, 1),
    exited = if_else(exited == "1", 1, 0)) %>%
  select(-geography)

pca_recipe <- recipe(exited ~ ., data = customer2) |>
              step_normalize(all_numeric_predictors()) |>
              step_pca(all_predictors(), threshold = .75)

# Prep the recipe to estimate PCA components

customer_prep_pca <- prep(pca_recipe, training = customer2)

# Extract the PCA results

customer_pca <- bake(customer_prep_pca, customer2) %>%
  mutate(
    exited = factor(exited))
```

We perform principal component analysis to reduce the number of features in the data set. The geography variable is categorical, and categorical variables cannot be represented well in PCA, so we remove it from our data set.

Overall, the data is not reduced well by Principal Component Analysis. In order to explain 75% of the variance in the data, we need to include 7 of the 9 components. These 7 components explain 82.41% of the data.

```{r PCA}
# View the results
tidy(customer_prep_pca, number = 2, type = "variance") %>%
  filter(terms == "cumulative percent variance") %>%
  ggplot() +
  geom_line(aes(x = component, y = value)) +
  scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9)) +
  geom_hline(yintercept = 75, linetype = "dashed") +
  labs(x = "Component", title = "Cumulative Percent Variance", y = "")
```

The percent variance explained by each individual component is obviously highest for component 1, and it slightly decreases for the next few components. The third through seventh components explain roughly the same amount of data. Overall the first seven components are enough to explain 82% of the data. These seven components are then used in the random forest model in Part Three.

```{r PCA}
tidy(customer_prep_pca, number = 2, type = "variance") %>%
  filter(terms == "percent variance") %>%
  ggplot() +
  geom_line(aes(x = component, y = value)) +
  scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9)) +
  labs(x = "Component", y = "Percent Variance Explained")
```

```{r PCA, include = FALSE}
# PCA Analysis

set.seed(1124)

dim(cust_dat)

pca_df <- cust_dat[,-c(2,3,8,9)]

dim(pca_df)

customer_pca <- princomp(pca_df[,-7], cor =T)

summary(customer_pca, loadings = T)

screeplot(customer_pca)


```

# Third Part

*Redo the method you used in part 1 but this time use the PCs found in part 2 (only the PCs that account for 75% of the variability). Again, comment on the accuracy and confusion matrix for both the training and testing sets.*

Now, we use our seven components to predict the customer exiting the market with a random forest model. As with before, we use 75% of the data in a training set and 25% in a testing set. We also use five fold cross validation to tune the mtry and min_n parameters in the model. This random forest has 500 trees.

The tuning selected mtry = and min_n = 

```{r pca rf}
set.seed(1128)
pca_split <- customer_pca %>% initial_split(prop = .75, strata = exited)

pca_train <- pca_split %>% training()

pca_test <- pca_split %>% testing()
pca_folds <- vfold_cv(pca_train, v = 5)

rf_spec <- rand_forest(trees = 500,
                       mtry = tune(),
                       min_n = tune()) %>%
  set_engine("randomforest", importance = TRUE) %>%
  set_mode("classification")

rf_pca_rec <- recipe(exited ~ ., data = pca_train) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())

rf_tuning_grid <- grid_regular(
  mtry(range = c(2, 4)),
  min_n(range = c(40, 50)),
  levels = 10)

rf_pca_tune_results <- tune_grid(
  object = workflow() %>%
    add_recipe(rf_pca_rec) %>%
    add_model(rf_spec),
  resamples = pca_folds,
  grid = rf_tuning_grid,
  metrics = metric_set(accuracy))

rf_pca_best_params <- select_best(rf_pca_tune_results, "accuracy")
rf_pca_best_params

rf_pca_final <- finalize_workflow(
  workflow() %>%
    add_recipe(rf_pca_rec) %>%
    add_model(rf_spec),
  best_params) %>%
  fit(data = pca_train)

predictions <- augment(pca_rf_final, new_data = pca_test)
```

```{r pca rf}
metrics(predictions, truth = exited, estimate = .pred_class)
conf_mat(predictions, truth = exited, estimate = .pred_class)

vip()
```



