# Caleb Skinner
# April 6, 2024
# Applied Time Series Analysis

# HW11 Part 1

# For the mtcars dataset, perform k-means clustering using all variables except
# for vs and am. For the number of clusters, tune using the same procedure shown
# in the notes. 

library("tidymodels")
library("tidyverse"); theme_set(theme_minimal())
library("tidyclust")
library("janitor")

# load data
cars <- mtcars %>% as_tibble() %>% clean_names()

# create recipe and remove vs and am variables
cars_recipe <- recipe(~., data = cars) %>%
  step_rm(vs, am) %>%
  step_normalize(all_numeric_predictors())

# create model and set seed
set.seed(1128)
kmeans_model <- k_means(num_clusters = tune()) %>%
  set_engine("stats")

# tuning grid
tuning_grid <- grid_regular(
  num_clusters(),
  levels = 20)

# create workflow
cars_wf <- workflow() %>%
  add_recipe(cars_recipe) %>%
  add_model(kmeans_model)

# set up cross validation, 4 folds because 4 divides into 32 evenly
cars_folds <- vfold_cv(cars, v = 4)

# tune model
res <- tune_cluster(
  cars_wf,
  resamples = cars_folds,
  grid = tuning_grid,
  control = control_grid(save_pred = TRUE, extract = identity),
  metrics = cluster_metric_set(sse_within_total, sse_total, sse_ratio))

# display results - sse over the ten clusters
res_metrics <- res %>% collect_metrics()

res_metrics %>%
  filter(.metric == "sse_ratio") %>%
  ggplot(aes(x = num_clusters, y = mean)) +
  geom_point() +
  geom_line() +
  labs(x = "Number of Clusters", y = "SSE Ratio") +
  scale_x_continuous(breaks = c(1:10))

# It appears to me that 3 clusters is the ideal selection. After three clusters,
# the reduction in the see ratio is minimal.

# new model with 3 clusters
kmeans_model3 <- k_means(num_clusters = 3) %>%
  set_engine("stats")

# new workflow
cars3_wf <- workflow() %>%
  add_recipe(cars_recipe) %>%
  add_model(kmeans_model3)

# model fit
fit <- cars3_wf %>% fit(data = cars)

# attach fit to data
cars_c <- augment(fit, cars)

# plot data on two dimensions
# mpg and drat seem to do well in visualizing the clusters
cars_c %>%
  ggplot(aes(x = mpg, y = drat, color = .pred_cluster)) +
  geom_point() +
  scale_color_discrete(name = "Cluster", labels = c("1", "2", "3"))

# We can do better by plotting the data on the first two principle components.
# We can then more easily compare this clustering to the clustering in part 2

pca_recipe <- recipe(~., data = cars) %>%
  step_rm(vs, am) %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors(), num_comp = 2)

cars_prep <- prep(pca_recipe, training = cars)

cars_pca <- bake(cars_prep, cars)

cars_c %>% bind_cols(cars_pca) %>%
  ggplot(aes(x = PC1, y = PC2, color = .pred_cluster)) +
  geom_point() +
  scale_color_discrete(name = "Cluster", labels = c("1", "2", "3"))


