---
title: "Project 2 Write Up"
author: Kevin Puorro, Asitha Mudiyanselage, Caleb Skinner
format:
  pdf:
    toc: true
---

```{r libraries}
library("tidyverse"); theme_set(theme_minimal())
library("tidymodels")
library("janitor")
library("xgboost")
library("vip")
library("flextable")
```

```{r, include = FALSE}
set_flextable_defaults(
  font.size = 10, theme_fun = theme_apa,
  padding = 6,
  background.color = "#EFEFEF")
```

\newpage

There are 10,000 records in this data set, consisting of six continuous variables, two categorical variables, and three numeric binary variables.

There is no high correlation between any variables, and there are no missing values in this data set. The scales of the credit_score and estimated_salary differ from the other variables. This indicates that scaling the data set is necessary before applying a machine learning model. Below we display the basic summary statistics of the data set.

```{r data}
customer <- read_csv("CustomerChurn.csv") %>%
  clean_names() %>%
  select(-customer_id, -surname) %>%
  mutate(
    exited = factor(exited),
    has_cr_card = factor(has_cr_card),
    is_active_member = factor(is_active_member)) %>%
  na.omit()

# split data
set.seed(1128)
customer_split <- initial_split(customer, prop = .75, strata = exited)
customer_train <- training(customer_split)
customer_test <- testing(customer_split)

# cross validation folds
customer_folds <- vfold_cv(customer_train, v = 5, strata = exited)
```

```{r basic summary statistics}

#Let's look at the basic statistics of the data set

# Remove  CustomerId and Surname

summary(customer)

# glimpse(dat)

#Let's look at the  correlation matrix

# cust_r <- cor(dat[,-c(2,3,8,9)])

# corrplot(cust_r, method = "number",number.cex = 0.7)

# Count missing values

# colSums(is.na(customer))

# Frequency count of the target (Exited) variable

# count(customer, exited) %>% 
#   flextable() %>%
#   align(align = "center")
```

\newpage

*Pick any machine learning method we have covered to predict ‘exited‘ based on the other variables (except customer_id and surname). Be sure to do a training and testing split. Whatever method you choose to use, be sure to tune the model. Comment on the accuracy and confusion matrix for both the training set and the testing set.*

# Random Forest

We use a classification random forest model to predict whether a customer will exit the market. After standardizing the predictor variables, we use 100 trees and tune the mtry and min_n parameters. The tuning methods selected an mtry value of 5 and a minimum node of 45.

```{r random forest}
rf_spec <- rand_forest(trees = 100,
                       mtry = tune(),
                       min_n = tune()) %>%
  set_engine("randomForest", importance = TRUE) %>%
  set_mode("classification")

rf_recipe <- recipe(exited ~ ., data = customer_train) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())

#specify the tuning grid
rf_tuning_grid <- grid_regular(
  mtry(range = c(3, 6)),
  min_n(range = c(40, 50)),
  levels = 10
)

#fine tune the model
rf_tune_results <- tune_grid(
  object = workflow() %>%
    add_recipe(rf_recipe) %>%
    add_model(rf_spec),
  resamples = customer_folds,
  grid = rf_tuning_grid,
  metrics = metric_set(accuracy)
)

rf_best_params <- select_best(rf_tune_results, "accuracy")
rf_best_params

rf_fitted_model <- finalize_workflow(
  workflow() %>%
    add_recipe(rf_recipe) %>%
    add_model(rf_spec),
  rf_best_params) %>%
  fit(data = customer_train)
```

```{r}
pred_train <- predict(rf_fitted_model, customer_train) %>%
  bind_cols(customer_train)

train_metrics <- metrics(pred_train, truth = exited, estimate = .pred_class)

train_conf_mat <- conf_mat(pred_train, truth = exited, estimate = .pred_class)

print(train_metrics)

print(train_conf_mat)
```

The model accurately classifies the exiting customer of the training set 89.54% of the time. The kappa estimate is 63.05%, showing that the model is not accurately predicting both outcomes at equal rates. The confusion matrix shows that most of the errors come when the model mislabels exiting customers as faithful ones. From the bank's perspective, this is concerning.

```{r}
predictions <- predict(rf_fitted_model, customer_test) %>%
  bind_cols(customer_test)

test_metrics <- metrics(predictions, truth = exited, estimate = .pred_class)

test_conf_mat <- conf_mat(predictions, truth = exited, estimate = .pred_class)

print(test_metrics)
print(test_conf_mat)
```

The model accurately classifies the exiting customer of the test set 86.4% of the time. The kappa estimate is 50.7%, showing that the model is not accurately predicting both outcomes at equal rates.

Lastly, we assessed each variable's importance in the final predictions. We found that the number of products and age are the most important variables in predicting if a customer will exit. Active membership, living in Germany, and balance are also very important predictors. The rest of the predictors seem to hold less overall value.

```{r}
vip(rf_fitted_model)
```

To ensure we found the most accurate model, we compare results from the Random Forest to that of several other models. Unsurprisingly, boosting (which is most similar to random forest) produces the second strongest accuracy. Logistic Regression performs decently well and the support vector machine is the least accurate.

```{r}
tibble(
  method = c("Random Forest", "Boosting", "Logistic Regression", "Support Vector Machine"),
  accuracy = c("86.4%", "83.5%", "82.1%", "79.6%")) %>%
  flextable() %>%
  align(align = "center", part = "all") %>%
  width(j = 1, width = 1.5)
```

\newpage

*Use Principal Component Analysis to reduce the number of features (again, do not use customer_id or surname). Choose only the number of PCs that capture 75% of the variability.*

# PCA Analysis

We perform principal component analysis to reduce the number of features in the data set. The geography variable is categorical, and categorical variables cannot be represented well in PCA, so we remove it from our data set.

```{r PCA dat}
# create pca df, converting binary variables into continuous
# remove geography because cannot be converted into ordinal values
customer2 <- customer %>%
  mutate(
    has_cr_card = if_else(has_cr_card == "1", 1, 0),
    is_active_member = if_else(is_active_member == "1", 1, 0),
    gender = if_else(gender == "Female", 0, 1),
    exited = if_else(exited == "1", 1, 0)) %>%
  select(-geography)
```

The "loadings" of each component signify how much each original variable contributes to the principal component. Hence, the balance and num_of_products have the highest loadings on PC1, indicating that they are the most influential variables in the variation captured by PC1. PC2 is dominated by the age and the is_active_member, and so forth.

```{r}

# PCA Analysis

set.seed(1124)

# dim(cust_dat)

# pca_df <- customer2[,-c(2,3,8,9)]

# dim(pca_df)

customer_pca <- customer2 %>% select(-exited) %>% princomp(cor =T)

summary(customer_pca, loadings = T)

# screeplot(customer_pca)
```

```{r PCA}
pca_recipe <- recipe(exited ~ ., data = customer2) |>
              step_normalize(all_numeric_predictors()) |>
              step_pca(all_predictors(), threshold = .75)

# Prep the recipe to estimate PCA components

customer_prep_pca <- prep(pca_recipe, training = customer2)

# Extract the PCA results

customer_pca <- bake(customer_prep_pca, customer2) %>%
  mutate(
    exited = factor(exited),
    geography = customer$geography)
```

Overall, the data is not reduced well by Principal Component Analysis. In order to explain 75% of the variance in the data, we need to include 7 of the 9 components. These 7 components explain 82.41% of the data.

```{r PCA2}
# View the results
tidy(customer_prep_pca, number = 2, type = "variance") %>%
  filter(terms == "cumulative percent variance") %>%
  ggplot() +
  geom_line(aes(x = component, y = value)) +
  scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9)) +
  geom_hline(yintercept = 75, linetype = "dashed") +
  labs(x = "Component", title = "Cumulative Percent Variance", y = "")
```

The percent variance explained by each individual component is obviously highest for component 1, and it slightly decreases for the next few components. The third through seventh components explain roughly the same amount of data. Overall the first seven components are enough to explain 82% of the data. These seven components are then used in the random forest model in Part Three.

```{r PCA3}
tidy(customer_prep_pca, number = 2, type = "variance") %>%
  filter(terms == "percent variance") %>%
  mutate(value = value/100) %>%
  ggplot() +
  geom_line(aes(x = component, y = value)) +
  scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9)) +
  labs(x = "Component", y = "Percent Variance Explained") +
  scale_y_continuous(labels = scales::label_percent())
```

# Third Part

*Redo the method you used in part 1 but this time use the PCs found in part 2 (only the PCs that account for 75% of the variability). Again, comment on the accuracy and confusion matrix for both the training and testing sets.*

Now, we use our seven components to predict the customer exiting the market with a random forest model. We added geography back into the data set, because geography was removed to perform the principal component analysis. As with before, we use 75% of the data in a training set and 25% in a testing set. We also use five fold cross validation to tune the mtry and min_n parameters in the model. This random forest has 500 trees.

The tuning selected mtry = 5 and min_n = 43.

```{r pca rf}
set.seed(1128)
pca_split <- customer_pca %>% initial_split(prop = .75, strata = exited)

pca_train <- pca_split %>% training()

pca_test <- pca_split %>% testing()
pca_folds <- vfold_cv(pca_train, v = 5)

rf_spec <- rand_forest(trees = 500,
                       mtry = tune(),
                       min_n = tune()) %>%
  set_engine("randomForest", importance = TRUE) %>%
  set_mode("classification")

rf_pca_rec <- recipe(exited ~ ., data = pca_train) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())

rf_tuning_grid <- grid_regular(
  mtry(range = c(4, 6)),
  min_n(range = c(40, 50)),
  levels = 10)

rf_pca_tune_results <- tune_grid(
  object = workflow() %>%
    add_recipe(rf_pca_rec) %>%
    add_model(rf_spec),
  resamples = pca_folds,
  grid = rf_tuning_grid,
  metrics = metric_set(accuracy))

rf_pca_best_params <- select_best(rf_pca_tune_results, "accuracy")
rf_pca_best_params

rf_pca_final <- finalize_workflow(
  workflow() %>%
    add_recipe(rf_pca_rec) %>%
    add_model(rf_spec),
  rf_pca_best_params) %>%
  fit(data = pca_train)

predictions <- augment(rf_pca_final, new_data = pca_test)
```

Overall, the model accurately classifies the exiting customer 82.41% of the time. The kappa estimate is 31.4%, showing that the model is not accurately predicting both outcomes at equal rates.

```{r}
metrics(predictions, truth = exited, estimate = .pred_class) %>%
  flextable() %>%
  align(align = "center") %>%
  colformat_double(j = 3, digits = 4)
```

The findings of the confusion matrix fits with the kappa estimate. The model accurately classifies 28.6% of the customers who exit. Of the 222 customers the model classifies as exiters, 146 (65.77%) actually exit.

```{r}
conf_mat(predictions, truth = exited, estimate = .pred_class)
```

We also looked at variable importance measurements for the model. This shows which variables are most significant in the classification of the customer. It's difficult to interpret the principal components, but it is interesting to see which components are most significant. THe most important two variables are PC1 and living in Germany.

```{r}
vip(rf_pca_final)
```
